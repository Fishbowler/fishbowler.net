---
title: "Marking your own work"
date: 2024-01-28
categories : ["learning", "testing"]
layout: post
draft: true
---

I recently read an essay written by my sister. She's on her second or third degree, training to become a teacher. She's worked in education for over a decade, but until I proof-read her recent essay, I had no worldly clue how much I didn't know about learning.

I wrote recently about Learning about Learning, and about how I manage the list of things I want to learn, and how I structure that time. I had no idea how much body of knowledge existed on the science of learning and of teaching, and how much of that you needed to know before becoming a teacher.

In her essay, it describes a recent handwriting intervention with some 8 year olds. They've fallen behind, and so the handwriting learning that they're doing in class isn't helping them because they haven't completed the foundational learning that the classwork is building on. In this session, the children were first engaged in a fine-motor function competition, reminded about how to form letters, went off to write lists of letters (something I remember my kids doing in early years - writing long strings of the same letter), but were then given those criteria for well-formed letters again and asked to judge which of their letters best fit those criteria.

The initial stage, practising related skills, is something we do in testing. We love testing games about pattern recognition. I've seen great talks on gamification within testing too, and how to do it right and wrong, and I remember the value of competing with other kids in the class and how that made me push myself.

The novel bit of this intervention was the self-assessment. I've led a lot of teams where I've promoted peer review of test planning to get the best collaborative plan, avoid obvious gaps, and to promote a bit of knowledge sharing about work and features. What I haven't done is reviewed my own testing after execution to see how well I achieved the value I was looking for, how closely it met the plan I had when I set out, and whether, if I was doing the testing again, would I do anything differently. What was the best testing I did that met the success criteria? What was the worst? If it was furthest from the criteria, was it a waste of time?

Sure, there's a meta-level to this. Who sets the success criteria? If it's only me, then shouldn't I always be able to succeed at them? Are the success criteria permanent, like "delivers risk information to the team?", or are they transient, like "validate user goals can be achieved for proof-of-concept - we'll go deeper for a Beta later"?

What I do know is that I'm going to spend more time looking backwards at my own testing, and marking my own work, rather than passing my set of intentions off to someone else and getting them to mark them for me.